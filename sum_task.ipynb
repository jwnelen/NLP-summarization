{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f3c0bfcc",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniconda/base/envs/NLP-summarization/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Reusing dataset cnn_dailymail (/Users/jeroen/.cache/huggingface/datasets/cnn_dailymail/3.0.0/3.0.0/3cb851bf7cf5826e45d49db2863f627cba583cbc32342df7349dfe6c38060234)\n",
      "100%|████████████████████████████████████████████| 3/3 [00:00<00:00, 132.49it/s]\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset, load_metric\n",
    "from transformers import pipeline\n",
    "\n",
    "raw_dataset = load_dataset('cnn_dailymail', '3.0.0')\n",
    "\n",
    "classifier = pipeline(\"summarization\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "32506305",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_examples(dataset, num_samples=3, seed=42):\n",
    "    samples = dataset[\"train\"].shuffle(seed=seed).select(range(num_samples))\n",
    "        \n",
    "    for idx, sample in enumerate(samples):\n",
    "        display(f'sample {idx}: {sample[\"article\"]} \\n')\n",
    "        display(f'highlight {idx}: {sample[\"highlights\"]} \\n')\n",
    "        display(f'id: {sample[\"id\"]}')\n",
    "        display('-------')\n",
    "        \n",
    "def get_samples(dataset, num_samples=10):\n",
    "    return dataset[\"train\"].shuffle(seed=1).select(range(num_samples))\n",
    "\n",
    "def get_random_sample(dataset):\n",
    "    sample = dataset[\"train\"].shuffle(seed=1).select(range(1)) \n",
    "    return [sample[\"article\"][0], sample[\"highlights\"][0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bb51370c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached shuffled indices for dataset at /Users/jeroen/.cache/huggingface/datasets/cnn_dailymail/3.0.0/3.0.0/3cb851bf7cf5826e45d49db2863f627cba583cbc32342df7349dfe6c38060234/cache-17d3e0cea13717ef.arrow\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'sample 0: WASHINGTON (CNN) -- The National Transportation Safety Board began four days of hearings Tuesday on how to stem the \"drastic increase\" in medical helicopter accidents. Smoke rises from Spectrum Health Butterworth Hospital in Grand Rapids, Michigan, after a helicopter crash in May. Over a recent 12-month period, the board probed nine fatal medical helicopter accidents that killed 35 people, a development that one board member called \"alarming.\" Medical helicopters \"provide an important service to the public\" in swiftly transporting ill and injured patients and donor organs, the board said on its Web site. Chopper pilots must operate \"safely and quickly\" in bad weather, at night or on \"unfamiliar landing sites,\" the board added. \"This hearing will be extremely important because it can provide an opportunity to learn more about the industry so that possibly we can make further recommendations that can prevent these accidents and save lives,\" said Robert Sumwalt, chairman of the hearing\\'s board of inquiry.  Watch Sumwalt\\'s remarks at hearing » . Flying at night in poor weather conditions likely contributed to the crashes in Texas and Alaska of three medical helicopters that killed 11 people, the NTSB said. The three crashes occurred near South Padre Island, Texas, in February 2008; Huntsville, Texas, in June; and Whittier, Alaska, in December 2007. iReport: Watch smoke pour from a medical chopper crash in Michigan . A December 2007 accident in Cherokee, Alabama, was likely caused by the pilot flying too low over trees, the NTSB said. The helicopter was shining a searchlight on a hunter who had been lost as rescue personnel on the ground tried to reach him. The pilot, a paramedic and a flight nurse were killed, the NTSB said. Among the issues to be discussed at the hearing will be flight operations, aircraft safety equipment, training and oversight. Expert witnesses such as pilots, medical personnel, managers and Federal Aviation Administration officials will give sworn testimony on what has been an \"ongoing concern\" of the safety board, which issued a report on emergency medical services operations in 2006. The NTSB said there were 55 EMS-related aviation accidents -- both fatal and nonfatal -- between January 2002 and January 2005 that could have been prevented with \"simple corrective actions.\" In those crashes, 54 people were killed, and 18 were seriously injured, the NTSB said. The agency recommended to the FAA in January 2006 that all medical chopper operators be required to develop and implement risk evaluation programs, use dispatch and flight procedures that include up-to-date weather information, and install \"terrain awareness and warning systems\" on their aircraft. A fourth recommendation would require medical flight operators to follow federal regulations regarding their flights. The recommendations have not been fully implemented, the NTSB said. \\n'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'highlight 0: Transportation safety board beginning four days of hearings .\\nBoard examines reported \"drastic increase\" in accidents and deaths .\\nNine air ambulance crashes killed 35 people during one-year period .\\nBoard\\'s 2006 safety recommendations not fully implemented, it says . \\n'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'id: 4929e54ae3f6711b4bd8da27a46d0f8a90c3b3bf'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'-------'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_examples(raw_dataset, 1, 23)\n",
    "# get_random_sample(raw_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5486444e",
   "metadata": {},
   "source": [
    "### Model description\n",
    "BART is a transformer encoder-encoder (seq2seq) model with a bidirectional (BERT-like) encoder and an autoregressive (GPT-like) decoder. BART is pre-trained by (1) corrupting text with an arbitrary noising function, and (2) learning a model to reconstruct the original text.\n",
    "\n",
    "BART is particularly effective when fine-tuned for text generation (e.g. summarization, translation) but also works well for comprehension tasks (e.g. text classification, question answering)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "898a49e4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "# A text-to-text transformer from Google\n",
    "# https://ai.googleblog.com/2020/02/exploring-transfer-learning-with-t5.html\n",
    "pretrained_model = \"t5-small\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(pretrained_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "81ce50e1",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached shuffled indices for dataset at /Users/jeroen/.cache/huggingface/datasets/cnn_dailymail/3.0.0/3.0.0/3cb851bf7cf5826e45d49db2863f627cba583cbc32342df7349dfe6c38060234/cache-d65aa25a11484207.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The', 'Ġdaughter', 'Ġof', 'Ġfootball', 'Ġlegend', 'ĠColin', 'ĠHend', 'ry', 'Ġhas', 'Ġspoken', 'Ġof', 'Ġher', 'Ġdisgust', 'Ġat', 'Ġthe', 'ĠGovernment', 'âĢ', 'Ļ', 's', 'Ġfailure', 'Ġto', 'Ġregulate', 'Ġcosmetic', 'Ġsurgeons', 'ĠâĢĵ', 'Ġmore', 'Ġthan', 'Ġa', 'Ġdecade', 'Ġafter', 'Ġher', 'Ġmother', 'Ġsuffered', 'Ġbotched', 'Ġlip', 'os', 'uction', 'Ġthat', 'Ġled', 'Ġto', 'Ġher', 'Ġdeath', '.', 'ĠDenise', 'ĠHend', 'ry', 'Ġdied', 'Ġin', 'Ġ2009', 'Ġaged', 'Ġ42', 'Ġfrom', 'Ġan', 'Ġinfection', 'Ġshe', 'Ġcaught', 'Ġduring', 'Ġsurgery', 'Ġto', 'Ġrebuild', 'Ġher', 'Ġstomach', '.', 'ĠShe', 'Ġhad', 'Ġbeen', 'Ġin', 'Ġand', 'Ġout', 'Ġof', 'Ġhospital', 'Ġfor', 'Ġseven', 'Ġyears', 'Ġafter', 'Ġa', 'Ġplastic', 'Ġsurgeon', 'Ġpunct', 'ured', 'Ġher', 'Ġbowel', 'Ġnine', 'Ġtimes', 'Ġin', 'Ġa', 'Ġroutine', 'Ġlip', 'os', 'uction', 'Ġoperation', 'Ġin', 'Ġ2002', '.', 'ĠAng', 'uish', ':', 'ĠColin', 'Ġand', 'ĠDenise', 'ĠHend', 'ry', 'Ġwith', 'ĠR', 'he', 'agan', 'Ġand', 'Ġher', 'Ġbrother', 'ĠKyle', 'Ġbefore', 'ĠDenise', \"'s\", 'Ġoperation', 'Ġ.', 'ĠOng', 'oing', 'Ġbattle', ':', 'ĠR', 'he', 'agan', 'ĠHend', 'ry', ',', 'Ġpictured', 'Ġwith', 'Ġher', 'Ġdaughter', 'ĠRiver', ',', 'Ġ2', ',', 'Ġis', 'Ġcampaigning', 'Ġfor', 'Ġbetter', 'Ġregulations', 'Ġafter', 'Ġher', 'Ġmother', 'ĠDenise', 'Ġdied', 'Ġof', 'Ġa', 'Ġbotched', 'Ġlip', 'os', 'uction', 'Ġoperation', 'Ġ.', 'ĠNow', 'Ġher', 'Ġdaughter', 'ĠR', 'he', 'agan', 'ĠHend', 'ry', 'Ġis', 'Ġlaunching', 'Ġa', 'Ġnew', 'ĠMail', 'Ġon', 'ĠSunday', 'Ġcampaign', 'Ġunder', 'Ġthe', 'Ġbanner', 'ĠStop', 'ĠThe', 'ĠCosmetic', 'ĠSurgery', 'ĠCowboys', '.', 'ĠR', 'he', 'agan', ',', 'Ġwho', 'Ġlives', 'Ġwith', 'Ġher', 'Ġtwo', '-', 'year', '-', 'old', 'Ġdaughter', 'ĠRiver', 'Ġin', 'ĠL', 'yth', 'am', 'ĠSt', 'ĠAn', 'nes', ',', 'ĠLanc', 'ash', 'ire', ',', 'Ġwas', 'Ġjust', 'Ġ12', 'Ġwhen', 'Ġher', 'Ġmother', 'Ġbecame', 'Ġill', '.', 'ĠIn', 'Ġher', 'Ġfirst', 'Ġmajor', 'Ġinterview', 'Ġsince', 'Ġher', 'Ġmother', 'Ġdied', ',', 'ĠR', 'he', 'agan', ',', 'Ġ23', ',', 'Ġsays', 'Ġclinic', 'Ġowners', 'ĠâĢ', 'ĺ', 'answer', 'Ġto', 'Ġno', 'Ġone', 'âĢ', 'Ļ', 'Ġand', 'Ġsurgery', 'Ġis', 'Ġbecoming', 'Ġever', 'Ġmore', 'Ġdangerous', '.', 'ĠShe', 'Ġis', 'Ġshocked', 'Ġthat', 'Ġrepeated', 'Ġwarnings', 'Ġfrom', 'Ġsenior', 'Ġmed', 'ics', ',', 'Ġincluding', 'Ġa', 'Ġformer', 'Ġchief', 'Ġmedical', 'Ġofficer', ',', 'Ġhave', 'Ġbeen', 'Ġignored', 'Ġby', 'ĠMinisters', '.', 'ĠâĢ', 'ĺ', 'How', 'Ġmany', 'Ġmore', 'Ġwomen', 'Ġneed', 'Ġto', 'Ġdie', 'Ġor', 'Ġbe', 'Ġdis', 'figured', 'Ġbefore', 'Ġsomeone', 'Ġtakes', 'Ġaction', '?', 'âĢ', 'Ļ', 'Ġsaid', 'ĠR', 'he', 'agan', ',', 'Ġa', 'Ġcare', 'r', 'Ġin', 'Ġa', 'Ġhome', 'Ġfor', 'Ġthe', 'Ġelderly', '.', 'ĠâĢ', 'ĺ', 'My', 'Ġmother', 'Ġcannot', 'Ġhave', 'Ġdied', 'Ġin', 'Ġvain', '.', 'âĢ', 'Ļ', 'ĠUnder', 'Ġcurrent', 'Ġrules', ',', 'Ġdoctors', 'Ġwho', 'Ġperform', 'Ġcosmetic', 'Ġoperations', 'Ġare', 'Ġnot', 'Ġrequired', 'Ġto', 'Ġhave', 'Ġany', 'Ġtraining', 'Ġin', 'Ġthe', 'Ġprocedures', 'Ġor', 'Ġeven', 'Ġto', 'Ġbe', 'Ġa', 'Ġsurgeon', '.', 'ĠForeign', 'Ġdoctors', 'Ġare', 'Ġable', 'Ġto', 'Ġfly', 'Ġin', ',', 'Ġcarry', 'Ġout', 'Ġoperations', 'Ġand', 'Ġfly', 'Ġout', 'Âł', 'Ġwithout', 'Ġproper', 'Ġinsurance', 'Ġor', 'Ġspecialist', 'Ġtraining', '.', 'ĠThe', 'ĠMail', 'Ġon', 'ĠSunday', 'Ġhas', 'Ġconsulted', 'Ġwith', 'Ġleading', 'Ġmedical', 'Ġorganisations', 'Ġand', 'Ġcompiled', 'Ġa', 'Ġfive', '-', 'point', 'Ġaction', 'Ġplan', 'Ġto', 'Ġensure', 'Ġpatients', 'Ġare', 'Ġprotected', 'Ġfrom', 'Ġthe', 'Ġcow', 'boys', '.', 'ĠWe', 'Ġare', 'Ġdemanding', 'Ġthat', ':', 'Ġ.', 'ĠFatal', 'Ġinfection', ':', 'ĠDenise', 'ĠHend', 'ry', 'Ġwith', 'Ġher', 'Ġformer', 'Ġfootballer', 'ĠColin', 'ĠHend', 'ry', '.', 'ĠDenise', 'Ġdied', 'Ġin', 'Ġ2009', 'Ġ.', 'ĠFormer', 'Ġchief', 'Ġmedical', 'Ġofficer', 'ĠSir', 'ĠLiam', 'ĠDonald', 'son', ',', 'Ġnow', 'Ġa', 'Ġprofessor', 'Ġat', 'ĠImperial', 'ĠCollege', 'ĠLondon', ',', 'Ġfirst', 'Ġcalled', 'Ġfor', 'Ġtougher', 'Ġregulations', 'Ġin', 'Ġ2006', '.', 'ĠLast', 'Ġnight', 'Ġhe', 'Ġsaid', ':', 'ĠâĢ', 'ĺ', 'I', 'Ġcommend', 'ĠThe', 'ĠMail', 'Ġon', 'ĠSunday', 'Ġon', 'Ġits', 'Ġcampaign', 'Ġto', 'Ġprotect', 'Ġpatients', 'Ġfrom', 'Ġpoor', 'Ġstandards', 'Ġof', 'Ġcosmetic', 'Ġtreatment', '.', 'âĢ', 'Ļ', 'ĠFormer', 'ĠHealth', 'ĠSecretary', 'ĠStephen', 'ĠDor', 'rell', 'Ġsaid', ':', 'ĠâĢ', 'ĺ', 'This', 'Ġcampaign', 'Ġhas', 'Âł', 'Ġthe', 'Ġright', 'Ġobjectives', 'Ġand', 'Ġsomething', 'Ġneeds', 'Ġto', 'Ġbe', 'Ġdone', '.', 'ĠI', 'Ġabsolutely', 'Ġagree', 'Ġthat', 'Ġanyone', 'Ġcarrying', 'Ġout', 'Ġcosmetic', 'Ġtraining', 'Ġmust', 'Ġbe', 'Ġproperly', 'Ġtrained', '.', 'âĢ', 'Ļ', 'ĠMr', 'ĠDor', 'rell', 'Ġadded', 'Ġthat', 'Ġcurrent', 'Ġregulators', 'Ġneeded', 'Ġto', 'Ġplay', 'Ġa', 'Ġtougher', 'Ġrole', '.', 'ĠAnn', 'ĠCl', 'w', 'yd', 'ĠMP', 'Ġwas', 'Ġappointed', 'Ġby', 'Ġthe', 'ĠPrime', 'ĠMinister', 'Ġas', 'Ġan', 'Ġadviser', 'Ġon', 'Ġpatient', 'Ġcomplaints', 'Ġand', 'Ġhas', 'Ġrecently', 'Ġlaunched', 'Ġa', 'Ġprivate', 'Ġmembers', 'âĢ', 'Ļ', 'ĠBill', ',', 'ĠThe', 'ĠCosmetic', 'ĠSurgery', 'ĠBill', ',', 'Ġto', 'Ġtry', 'Ġto', 'Ġtackle', 'Ġthe', 'Ġissue', '.', 'ĠShe', 'Ġsaid', ':', 'ĠâĢ', 'ĺ', 'I', 'Ġam', 'Ġabsolutely', 'Ġdelighted', 'Ġby', 'ĠThe', 'ĠMail', 'Ġon', 'ĠSunday', 'Ġcampaign', 'Ġand', 'ĠI', 'Ġfully', 'Ġsupport', 'Ġit', '.', 'âĢ', 'Ļ', 'ĠIn', 'Ġrecent', 'Ġyears', ',', 'Ġthe', 'Ġnumber', 'Ġof', 'Ġpeople', 'Ġopting', 'Ġfor', 'Ġcosmetic', 'Ġsurgery', ',', 'Ġincluding', 'Ġnose', 'Ġjobs', ',', 'Ġbreast', 'Ġenlarg', 'ements', 'Ġand', 'Ġlip', 'os', 'uction', ',', 'Ġhas', 'Ġspir', 'alled', '.', 'ĠThe', 'ĠRoyal', 'ĠCollege', 'Ġof', 'ĠSur', 'geons', 'Ġestimates', 'Ġthat', 'Ġlast', 'Ġyear', 'Ġalone', 'Ġ130', ',', '000', 'Ġoperations', 'Ġwere', 'Ġcarried', 'Ġout', ',', 'Ġup', 'Ġfive', 'Ġper', 'Ġcent', 'Ġon', 'Ġthe', 'Ġprevious', 'Ġyear', '.', 'ĠAlthough', 'Ġmany', 'Ġsurgeons', 'Ġundergo', 'Ġextra', 'Ġtraining', 'Ġto', 'Ġensure', 'Ġthey', 'Ġknow', 'Ġhow', 'Ġto', 'Ġdo', 'Ġcosmetic', 'Ġprocedures', 'Ġproperly', ',', 'Ġthis', 'Ġis', 'Ġnot', 'Ġrequired', 'Ġby', 'Ġlaw', 'ĠâĢĵ', 'Ġand', 'Ġin', 'Ġmany', 'Ġcases', ',', 'Ġdoctors', 'Ġare', 'Ġnot', 'Ġskilled', 'Ġin', 'Ġthe', 'Ġprocedures', 'Ġthey', 'Ġoffer', '.', 'ĠLast', 'Ġyear', 'Ġthe', 'ĠGovernment', 'Ġordered', 'Ġa', 'Ġreview', 'Ġof', 'Ġcosmetic', 'Ġsurgery', 'Âł', 'Ġfollowing', 'Ġthe', 'ĠP', 'IP', 'Ġbreast', 'Ġimplant', 'Ġscandal', '.', 'ĠThe', 'Ġreport', 'Ġby', 'Ġthe', 'ĠDepartment', 'Ġof', 'ĠHealth', 'âĢ', 'Ļ', 's', 'Ġmedical', 'Ġdirector', ',', 'ĠProfessor', 'ĠSir', 'ĠBruce', 'ĠKe', 'ogh', ',', 'Ġis', 'Ġdue', 'Ġto', 'Ġbe', 'Ġpublished', 'Ġshortly', '.']\n"
     ]
    }
   ],
   "source": [
    "article, highlight = get_random_sample(raw_dataset)\n",
    "print(tokenizer.tokenize(article))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "22b998c2",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Metric(name: \"rouge\", features: {'predictions': Value(dtype='string', id='sequence'), 'references': Value(dtype='string', id='sequence')}, usage: \"\"\"\n",
       "Calculates average rouge scores for a list of hypotheses and references\n",
       "Args:\n",
       "    predictions: list of predictions to score. Each predictions\n",
       "        should be a string with tokens separated by spaces.\n",
       "    references: list of reference for each prediction. Each\n",
       "        reference should be a string with tokens separated by spaces.\n",
       "    rouge_types: A list of rouge types to calculate.\n",
       "        Valid names:\n",
       "        `\"rouge{n}\"` (e.g. `\"rouge1\"`, `\"rouge2\"`) where: {n} is the n-gram based scoring,\n",
       "        `\"rougeL\"`: Longest common subsequence based scoring.\n",
       "        `\"rougeLSum\"`: rougeLsum splits text using `\"\n",
       "\"`.\n",
       "        See details in https://github.com/huggingface/datasets/issues/617\n",
       "    use_stemmer: Bool indicating whether Porter stemmer should be used to strip word suffixes.\n",
       "    use_agregator: Return aggregates if this is set to True\n",
       "Returns:\n",
       "    rouge1: rouge_1 (precision, recall, f1),\n",
       "    rouge2: rouge_2 (precision, recall, f1),\n",
       "    rougeL: rouge_l (precision, recall, f1),\n",
       "    rougeLsum: rouge_lsum (precision, recall, f1)\n",
       "Examples:\n",
       "\n",
       "    >>> rouge = datasets.load_metric('rouge')\n",
       "    >>> predictions = [\"hello there\", \"general kenobi\"]\n",
       "    >>> references = [\"hello there\", \"general kenobi\"]\n",
       "    >>> results = rouge.compute(predictions=predictions, references=references)\n",
       "    >>> print(list(results.keys()))\n",
       "    ['rouge1', 'rouge2', 'rougeL', 'rougeLsum']\n",
       "    >>> print(results[\"rouge1\"])\n",
       "    AggregateScore(low=Score(precision=1.0, recall=1.0, fmeasure=1.0), mid=Score(precision=1.0, recall=1.0, fmeasure=1.0), high=Score(precision=1.0, recall=1.0, fmeasure=1.0))\n",
       "    >>> print(results[\"rouge1\"].mid.fmeasure)\n",
       "    1.0\n",
       "\"\"\", stored examples: 0)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metric = load_metric(\"rouge\")\n",
    "metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "eae5124f",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_input_length = 1024\n",
    "max_target_length = 64\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    model_inputs = tokenizer(\n",
    "        examples[\"article\"], max_length=max_input_length, truncation=True\n",
    "    )\n",
    "    # Set up the tokenizer for targets\n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        labels = tokenizer(\n",
    "            examples[\"highlights\"], max_length=max_target_length, truncation=True\n",
    "        )\n",
    "\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "37646611",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached shuffled indices for dataset at /Users/jeroen/.cache/huggingface/datasets/cnn_dailymail/3.0.0/3.0.0/3cb851bf7cf5826e45d49db2863f627cba583cbc32342df7349dfe6c38060234/cache-d65aa25a11484207.arrow\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'T5Tokenizer' object has no attribute 'as_target_tokenizer'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[0;32mIn [23]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m model_inputs \u001b[38;5;241m=\u001b[39m \u001b[43mpreprocess_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43mget_samples\u001b[49m\u001b[43m(\u001b[49m\u001b[43mraw_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[0;32mIn [22]\u001b[0m, in \u001b[0;36mpreprocess_function\u001b[0;34m(examples)\u001b[0m\n\u001b[1;32m      5\u001b[0m model_inputs \u001b[38;5;241m=\u001b[39m tokenizer(\n\u001b[1;32m      6\u001b[0m     examples[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marticle\u001b[39m\u001b[38;5;124m\"\u001b[39m], max_length\u001b[38;5;241m=\u001b[39mmax_input_length, truncation\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m      7\u001b[0m )\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# Set up the tokenizer for targets\u001b[39;00m\n\u001b[0;32m----> 9\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mas_target_tokenizer\u001b[49m():\n\u001b[1;32m     10\u001b[0m     labels \u001b[38;5;241m=\u001b[39m tokenizer(\n\u001b[1;32m     11\u001b[0m         examples[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhighlights\u001b[39m\u001b[38;5;124m\"\u001b[39m], max_length\u001b[38;5;241m=\u001b[39mmax_target_length, truncation\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m     12\u001b[0m     )\n\u001b[1;32m     14\u001b[0m model_inputs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabels\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m labels[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'T5Tokenizer' object has no attribute 'as_target_tokenizer'"
     ]
    }
   ],
   "source": [
    "model_inputs = preprocess_function(get_samples(raw_dataset, 20))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NLP-summarization",
   "language": "python",
   "name": "nlp-summarization"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
