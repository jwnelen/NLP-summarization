{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "13b17139",
   "metadata": {},
   "source": [
    "Model https://huggingface.co/mrm8488/bert-mini2bert-mini-finetuned-cnn_daily_mail-summarization\n",
    "\n",
    "Original colab https://colab.research.google.com/drive/1Ekd5pUeCX7VOrMx94_czTkwNtLN32Uyu?usp=sharing#scrollTo=oOoSrwWarJAC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bc3c6bed",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "# Judging from the original colab this resolves to BertTokenizer \n",
    "tokenizer = AutoTokenizer.from_pretrained(\"mrm8488/bert-mini2bert-mini-finetuned-cnn_daily_mail-summarization\")\n",
    "\n",
    "# Judging from the original colab this resolves to EncoderDecoderModel\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"mrm8488/bert-mini2bert-mini-finetuned-cnn_daily_mail-summarization\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b534e915",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset cnn_dailymail (/Users/jeroen/.cache/huggingface/datasets/cnn_dailymail/3.0.0/3.0.0/3cb851bf7cf5826e45d49db2863f627cba583cbc32342df7349dfe6c38060234)\n",
      "100%|███████████████████████████████████████████| 25/25 [03:34<00:00,  8.59s/ba]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score(precision=0.08951373193489645, recall=0.14201186631096696, fmeasure=0.10703464255630432)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import datasets\n",
    "from transformers import BertTokenizer, EncoderDecoderModel\n",
    "rouge = datasets.load_metric(\"rouge\")\n",
    "\n",
    "test_data = datasets.load_dataset(\"cnn_dailymail\", \"3.0.0\", split=\"test\")\n",
    "\n",
    "# only use 16 training examples for notebook - DELETE LINE FOR FULL TRAINING\n",
    "r = 400\n",
    "test_data = test_data.select(range(r))\n",
    "\n",
    "batch_size = 16 # change to 64 for full evaluation\n",
    "\n",
    "# map data correctly\n",
    "def generate_summary(batch):\n",
    "    # Tokenizer will automatically set [BOS] <text> [EOS]\n",
    "    # cut off at BERT max length 512\n",
    "    inputs = tokenizer(batch[\"article\"], padding=\"max_length\", truncation=True, max_length=512, return_tensors=\"pt\")\n",
    "    input_ids = inputs.input_ids#.to(\"cuda\")\n",
    "    attention_mask = inputs.attention_mask#.to(\"cuda\")\n",
    "\n",
    "    outputs = model.generate(input_ids, attention_mask=attention_mask)\n",
    "\n",
    "    # all special tokens including will be removed\n",
    "    output_str = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "\n",
    "    batch[\"pred\"] = output_str\n",
    "\n",
    "    return batch\n",
    "\n",
    "results = test_data.map(generate_summary, batched=True, batch_size=batch_size, remove_columns=[\"article\"])\n",
    "\n",
    "pred_str = results[\"pred\"]\n",
    "label_str = results[\"highlights\"]\n",
    "\n",
    "rouge_output = rouge.compute(predictions=pred_str, references=label_str, rouge_types=[\"rouge2\"])[\"rouge2\"].mid\n",
    "\n",
    "print(rouge_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8ceebff2",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/np/sw3cwd7x6rv0llrv26d2w1s40000gn/T/ipykernel_4070/3014298243.py:2: DeprecationWarning: Importing display from IPython.core.display is deprecated since IPython 7.14, please import from IPython display\n",
      "  from IPython.core.display import display, HTML\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 're' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [7]\u001b[0m, in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mIPython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdisplay\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m display, HTML\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m10\u001b[39m):\n\u001b[0;32m----> 5\u001b[0m     match \u001b[38;5;241m=\u001b[39m \u001b[43mdisplay_string_matching\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresults\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mhighlights\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresults\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpred\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mboth\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msentences\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtitles\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mReal Summary\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mPredicted Summary\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m     display(HTML(match))\n",
      "Input \u001b[0;32mIn [6]\u001b[0m, in \u001b[0;36mdisplay_string_matching\u001b[0;34m(a, b, both, sentences, titles)\u001b[0m\n\u001b[1;32m     76\u001b[0m first_text \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     77\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m lst_a:\n\u001b[0;32m---> 78\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mre\u001b[49m\u001b[38;5;241m.\u001b[39msub(\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m[^\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124ms]\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m, i\u001b[38;5;241m.\u001b[39mlower()) \u001b[38;5;129;01min\u001b[39;00m [re\u001b[38;5;241m.\u001b[39msub(\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m[^\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124ms]\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m, z\u001b[38;5;241m.\u001b[39mlower()) \u001b[38;5;28;01mfor\u001b[39;00m z \u001b[38;5;129;01min\u001b[39;00m lst_b]:\n\u001b[1;32m     79\u001b[0m         first_text\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m<span style=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbackground-color:rgba(255,215,0,0.3);\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m>\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m i \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m</span>\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     80\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mNameError\u001b[0m: name 're' is not defined"
     ]
    }
   ],
   "source": [
    "# Just showing some of the results\n",
    "from IPython.core.display import display, HTML\n",
    "\n",
    "for i in range(10):\n",
    "    match = display_string_matching(results[\"highlights\"][i], results[\"pred\"][i], both=True, sentences=False, titles=[\"Real Summary\", \"Predicted Summary\"])\n",
    "    display(HTML(match))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a6a30ea5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Credits tot Mauro Di Pietro\n",
    "# https://towardsdatascience.com/text-summarization-with-nlp-textrank-vs-seq2seq-vs-bart-474943efeb09\n",
    "'''\n",
    "Find the matching substrings in 2 strings.\n",
    ":parameter\n",
    "    :param a: string - raw text\n",
    "    :param b: string - raw text\n",
    ":return\n",
    "    2 lists used in to display matches\n",
    "'''\n",
    "def utils_split_sentences(a, b):\n",
    "    ## find clean matches\n",
    "    match = difflib.SequenceMatcher(isjunk=None, a=a, b=b, autojunk=True)\n",
    "    lst_match = [block for block in match.get_matching_blocks() if block.size > 20]\n",
    "    \n",
    "    ## difflib didn't find any match\n",
    "    if len(lst_match) == 0:\n",
    "        lst_a, lst_b = nltk.sent_tokenize(a), nltk.sent_tokenize(b)\n",
    "    \n",
    "    ## work with matches\n",
    "    else:\n",
    "        first_m, last_m = lst_match[0], lst_match[-1]\n",
    "\n",
    "        ### a\n",
    "        string = a[0 : first_m.a]\n",
    "        lst_a = [t for t in nltk.sent_tokenize(string)]\n",
    "        for n in range(len(lst_match)):\n",
    "            m = lst_match[n]\n",
    "            string = a[m.a : m.a+m.size]\n",
    "            lst_a.append(string)\n",
    "            if n+1 < len(lst_match):\n",
    "                next_m = lst_match[n+1]\n",
    "                string = a[m.a+m.size : next_m.a]\n",
    "                lst_a = lst_a + [t for t in nltk.sent_tokenize(string)]\n",
    "            else:\n",
    "                break\n",
    "        string = a[last_m.a+last_m.size :]\n",
    "        lst_a = lst_a + [t for t in nltk.sent_tokenize(string)]\n",
    "\n",
    "        ### b\n",
    "        string = b[0 : first_m.b]\n",
    "        lst_b = [t for t in nltk.sent_tokenize(string)]\n",
    "        for n in range(len(lst_match)):\n",
    "            m = lst_match[n]\n",
    "            string = b[m.b : m.b+m.size]\n",
    "            lst_b.append(string)\n",
    "            if n+1 < len(lst_match):\n",
    "                next_m = lst_match[n+1]\n",
    "                string = b[m.b+m.size : next_m.b]\n",
    "                lst_b = lst_b + [t for t in nltk.sent_tokenize(string)]\n",
    "            else:\n",
    "                break\n",
    "        string = b[last_m.b+last_m.size :]\n",
    "        lst_b = lst_b + [t for t in nltk.sent_tokenize(string)]\n",
    "    \n",
    "    return lst_a, lst_b\n",
    "\n",
    "\n",
    "'''\n",
    "Highlights the matched strings in text.\n",
    ":parameter\n",
    "    :param a: string - raw text\n",
    "    :param b: string - raw text\n",
    "    :param both: bool - search a in b and, if True, viceversa\n",
    "    :param sentences: bool - if False matches single words\n",
    ":return\n",
    "    text html, it can be visualized on notebook with display(HTML(text))\n",
    "'''\n",
    "def display_string_matching(a, b, both=True, sentences=True, titles=[]):\n",
    "    if sentences is True:\n",
    "        lst_a, lst_b = utils_split_sentences(a, b)\n",
    "    else:\n",
    "        lst_a, lst_b = a.split(), b.split()       \n",
    "    \n",
    "    ## highlight a\n",
    "    first_text = []\n",
    "    for i in lst_a:\n",
    "        if re.sub(r'[^\\w\\s]', '', i.lower()) in [re.sub(r'[^\\w\\s]', '', z.lower()) for z in lst_b]:\n",
    "            first_text.append('<span style=\"background-color:rgba(255,215,0,0.3);\">' + i + '</span>')\n",
    "        else:\n",
    "            first_text.append(i)\n",
    "    first_text = ' '.join(first_text)\n",
    "    \n",
    "    ## highlight b\n",
    "    second_text = []\n",
    "    if both is True:\n",
    "        for i in lst_b:\n",
    "            if re.sub(r'[^\\w\\s]', '', i.lower()) in [re.sub(r'[^\\w\\s]', '', z.lower()) for z in lst_a]:\n",
    "                second_text.append('<span style=\"background-color:rgba(255,215,0,0.3);\">' + i + '</span>')\n",
    "            else:\n",
    "                second_text.append(i)\n",
    "    else:\n",
    "        second_text.append(b) \n",
    "    second_text = ' '.join(second_text)\n",
    "    \n",
    "    ## concatenate\n",
    "    if len(titles) > 0:\n",
    "        first_text = \"<strong>\"+titles[0]+\"</strong><br>\"+first_text\n",
    "    if len(titles) > 1:\n",
    "        second_text = \"<strong>\"+titles[1]+\"</strong><br>\"+second_text\n",
    "    else:\n",
    "        second_text = \"---\"*65+\"<br><br>\"+second_text\n",
    "    final_text = first_text +'<br><br>'+ second_text\n",
    "    return final_text"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NLP-summarization",
   "language": "python",
   "name": "nlp-summarization"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
