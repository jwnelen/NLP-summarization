{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2688b57c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets\n",
    "import transformers\n",
    "import rouge"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "245e9284",
   "metadata": {},
   "source": [
    "### Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7e6f6c00",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset cnn_dailymail (/Users/jeroen/.cache/huggingface/datasets/cnn_dailymail/3.0.0/3.0.0/3cb851bf7cf5826e45d49db2863f627cba583cbc32342df7349dfe6c38060234)\n",
      "Reusing dataset cnn_dailymail (/Users/jeroen/.cache/huggingface/datasets/cnn_dailymail/3.0.0/3.0.0/3cb851bf7cf5826e45d49db2863f627cba583cbc32342df7349dfe6c38060234)\n",
      "Reusing dataset cnn_dailymail (/Users/jeroen/.cache/huggingface/datasets/cnn_dailymail/3.0.0/3.0.0/3cb851bf7cf5826e45d49db2863f627cba583cbc32342df7349dfe6c38060234)\n"
     ]
    }
   ],
   "source": [
    "train_data =       datasets.load_dataset(\"cnn_dailymail\", \"3.0.0\", split=\"train\")\n",
    "validation_data =  datasets.load_dataset(\"cnn_dailymail\", \"3.0.0\", split=\"validation\")\n",
    "test_data =        datasets.load_dataset(\"cnn_dailymail\", \"3.0.0\", split=\"test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7864de49",
   "metadata": {},
   "source": [
    "### Sample of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8b7119e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_examples(dataset, num_samples=3, seed=42):\n",
    "    samples = dataset.shuffle(seed=seed).select(range(num_samples))\n",
    "        \n",
    "    for idx, sample in enumerate(samples):\n",
    "        display(f'sample {idx}: {sample[\"article\"]} \\n')\n",
    "        display(f'highlight {idx}: {sample[\"highlights\"]} \\n')\n",
    "        display(f'id: {sample[\"id\"]}')\n",
    "        display('-------')\n",
    "        \n",
    "def get_samples(dataset, num_samples=10):\n",
    "    return dataset.shuffle(seed=1).select(range(num_samples))\n",
    "\n",
    "def get_random_sample(dataset):\n",
    "    sample = dataset.shuffle(seed=1).select(range(1)) \n",
    "    return [sample[\"article\"][0], sample[\"highlights\"][0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "207df897",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# get_random_sample(train_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f082f3e",
   "metadata": {},
   "source": [
    "### Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c29f5588",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file https://huggingface.co/distilbert-base-uncased/resolve/main/config.json from cache at /Users/jeroen/.cache/huggingface/transformers/23454919702d26495337f3da04d1655c7ee010d5ec9d77bdb9e399e00302c0a1.91b885ab15d631bf9cee9dc9d25ece0afd932f2f5130eba28f2055b2220c0333\n",
      "Model config DistilBertConfig {\n",
      "  \"_name_or_path\": \"distilbert-base-uncased\",\n",
      "  \"activation\": \"gelu\",\n",
      "  \"architectures\": [\n",
      "    \"DistilBertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"dim\": 768,\n",
      "  \"dropout\": 0.1,\n",
      "  \"hidden_dim\": 3072,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"distilbert\",\n",
      "  \"n_heads\": 12,\n",
      "  \"n_layers\": 6,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"qa_dropout\": 0.1,\n",
      "  \"seq_classif_dropout\": 0.2,\n",
      "  \"sinusoidal_pos_embds\": false,\n",
      "  \"tie_weights_\": true,\n",
      "  \"transformers_version\": \"4.17.0.dev0\",\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading file https://huggingface.co/distilbert-base-uncased/resolve/main/vocab.txt from cache at /Users/jeroen/.cache/huggingface/transformers/0e1bbfda7f63a99bb52e3915dcf10c3c92122b827d92eb2d34ce94ee79ba486c.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99\n",
      "loading file https://huggingface.co/distilbert-base-uncased/resolve/main/tokenizer.json from cache at /Users/jeroen/.cache/huggingface/transformers/75abb59d7a06f4f640158a9bfcde005264e59e8d566781ab1415b139d2e4c603.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4\n",
      "loading file https://huggingface.co/distilbert-base-uncased/resolve/main/added_tokens.json from cache at None\n",
      "loading file https://huggingface.co/distilbert-base-uncased/resolve/main/special_tokens_map.json from cache at None\n",
      "loading file https://huggingface.co/distilbert-base-uncased/resolve/main/tokenizer_config.json from cache at /Users/jeroen/.cache/huggingface/transformers/8c8624b8ac8aa99c60c912161f8332de003484428c47906d7ff7eb7f73eecdbb.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79\n",
      "loading configuration file https://huggingface.co/distilbert-base-uncased/resolve/main/config.json from cache at /Users/jeroen/.cache/huggingface/transformers/23454919702d26495337f3da04d1655c7ee010d5ec9d77bdb9e399e00302c0a1.91b885ab15d631bf9cee9dc9d25ece0afd932f2f5130eba28f2055b2220c0333\n",
      "Model config DistilBertConfig {\n",
      "  \"_name_or_path\": \"distilbert-base-uncased\",\n",
      "  \"activation\": \"gelu\",\n",
      "  \"architectures\": [\n",
      "    \"DistilBertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"dim\": 768,\n",
      "  \"dropout\": 0.1,\n",
      "  \"hidden_dim\": 3072,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"distilbert\",\n",
      "  \"n_heads\": 12,\n",
      "  \"n_layers\": 6,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"qa_dropout\": 0.1,\n",
      "  \"seq_classif_dropout\": 0.2,\n",
      "  \"sinusoidal_pos_embds\": false,\n",
      "  \"tie_weights_\": true,\n",
      "  \"transformers_version\": \"4.17.0.dev0\",\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "144\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████| 36/36 [00:00<00:00, 249.78ba/s]\n",
      "100%|████████████████████████████████████████████| 2/2 [00:00<00:00, 246.31ba/s]\n"
     ]
    }
   ],
   "source": [
    "batch_size=4 # change to 16 for full training\n",
    "encoder_max_length=512\n",
    "decoder_max_length=128\n",
    "\n",
    "tokenizer = transformers.AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "# tokenizer = transformers.DistilBertTokenizerFast()\n",
    "tokenizer.bos_token = tokenizer.cls_token\n",
    "tokenizer.eos_token = tokenizer.sep_token\n",
    "\n",
    "def tokenize_data_to_model_input(batch):\n",
    "    inputs  = tokenizer(batch[\"article\"], padding=\"max_length\", \n",
    "                       truncation=True, max_length=encoder_max_length)\n",
    "    \n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        labels = tokenizer(batch[\"highlights\"], \n",
    "                           padding=\"max_length\", truncation=True, max_length=decoder_max_length)\n",
    "    \n",
    "    batch[\"input_ids\"] = inputs.input_ids\n",
    "    batch[\"attention_mask\"] = inputs.attention_mask\n",
    "    \n",
    "    batch[\"decoder_input_ids\"] = labels.input_ids\n",
    "    batch[\"decoder_attention_mask\"] = labels.attention_mask\n",
    "    batch[\"labels\"] = labels[\"input_ids\"]\n",
    "    \n",
    "    # because BERT automatically shifts the labels, the labels correspond exactly to `decoder_input_ids`. \n",
    "    # We have to make sure that the PAD token is ignored\n",
    "    batch[\"labels\"] = [[-100 if token == tokenizer.pad_token_id else token for token in labels] for labels in batch[\"labels\"]]\n",
    "    \n",
    "    return batch\n",
    "\n",
    "# For now subsample is being used\n",
    "percentile = 0.0005\n",
    "amount = round(len(train_data) * percentile)\n",
    "print(amount)\n",
    "train_data = train_data.select(range(amount))\n",
    "\n",
    "train_data = train_data.map(\n",
    "    tokenize_data_to_model_input, \n",
    "    batched=True, \n",
    "    batch_size=batch_size, \n",
    "    remove_columns=[\"article\", \"highlights\", \"id\"]\n",
    ")\n",
    "\n",
    "train_data.set_format(\n",
    "    type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"decoder_input_ids\", \"decoder_attention_mask\", \"labels\"],\n",
    ")\n",
    "\n",
    "\n",
    "# only use 16 training examples for notebook - DELETE LINE FOR FULL TRAINING\n",
    "amount = round(len(validation_data) * percentile)\n",
    "validation_data = validation_data.select(range(amount))\n",
    "\n",
    "validation_data = validation_data.map(\n",
    "    tokenize_data_to_model_input, \n",
    "    batched=True, \n",
    "    batch_size=batch_size, \n",
    "    remove_columns=[\"article\", \"highlights\", \"id\"]\n",
    ")\n",
    "validation_data.set_format(\n",
    "    type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"decoder_input_ids\", \"decoder_attention_mask\", \"labels\"],\n",
    ")\n",
    "\n",
    "\n",
    "test_data = test_data.select(range(128))\n",
    "# test_data = test_data.map(\n",
    "#     tokenize_data_to_model_input, \n",
    "#     batched=True, \n",
    "#     batch_size=batch_size, \n",
    "#     remove_columns=[\"article\", \"highlights\", \"id\"]\n",
    "# )\n",
    "# test_data.set_format(type=\"torch\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8dd7c7c",
   "metadata": {},
   "source": [
    "### Encoder - Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "67ad59cc",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file https://huggingface.co/distilbert-base-uncased/resolve/main/config.json from cache at /Users/jeroen/.cache/huggingface/transformers/23454919702d26495337f3da04d1655c7ee010d5ec9d77bdb9e399e00302c0a1.91b885ab15d631bf9cee9dc9d25ece0afd932f2f5130eba28f2055b2220c0333\n",
      "Model config DistilBertConfig {\n",
      "  \"_name_or_path\": \"distilbert-base-uncased\",\n",
      "  \"activation\": \"gelu\",\n",
      "  \"architectures\": [\n",
      "    \"DistilBertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"dim\": 768,\n",
      "  \"dropout\": 0.1,\n",
      "  \"hidden_dim\": 3072,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"distilbert\",\n",
      "  \"n_heads\": 12,\n",
      "  \"n_layers\": 6,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"qa_dropout\": 0.1,\n",
      "  \"seq_classif_dropout\": 0.2,\n",
      "  \"sinusoidal_pos_embds\": false,\n",
      "  \"tie_weights_\": true,\n",
      "  \"transformers_version\": \"4.17.0.dev0\",\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/distilbert-base-uncased/resolve/main/pytorch_model.bin from cache at /Users/jeroen/.cache/huggingface/transformers/9c169103d7e5a73936dd2b627e42851bec0831212b677c637033ee4bce9ab5ee.126183e36667471617ae2f0835fab707baa54b731f991507ebbb55ea85adb12a\n",
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_projector.bias', 'vocab_layer_norm.bias', 'vocab_transform.weight', 'vocab_projector.weight', 'vocab_layer_norm.weight', 'vocab_transform.bias']\n",
      "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of DistilBertModel were initialized from the model checkpoint at distilbert-base-uncased.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use DistilBertModel for predictions without further training.\n",
      "loading configuration file https://huggingface.co/distilbert-base-uncased/resolve/main/config.json from cache at /Users/jeroen/.cache/huggingface/transformers/23454919702d26495337f3da04d1655c7ee010d5ec9d77bdb9e399e00302c0a1.91b885ab15d631bf9cee9dc9d25ece0afd932f2f5130eba28f2055b2220c0333\n",
      "Model config DistilBertConfig {\n",
      "  \"_name_or_path\": \"distilbert-base-uncased\",\n",
      "  \"activation\": \"gelu\",\n",
      "  \"architectures\": [\n",
      "    \"DistilBertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"dim\": 768,\n",
      "  \"dropout\": 0.1,\n",
      "  \"hidden_dim\": 3072,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"distilbert\",\n",
      "  \"n_heads\": 12,\n",
      "  \"n_layers\": 6,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"qa_dropout\": 0.1,\n",
      "  \"seq_classif_dropout\": 0.2,\n",
      "  \"sinusoidal_pos_embds\": false,\n",
      "  \"tie_weights_\": true,\n",
      "  \"transformers_version\": \"4.17.0.dev0\",\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "Initializing distilbert-base-uncased as a decoder model. Cross attention layers are added to distilbert-base-uncased and randomly initialized if distilbert-base-uncased's architecture allows for cross attention layers.\n",
      "loading weights file https://huggingface.co/distilbert-base-uncased/resolve/main/pytorch_model.bin from cache at /Users/jeroen/.cache/huggingface/transformers/9c169103d7e5a73936dd2b627e42851bec0831212b677c637033ee4bce9ab5ee.126183e36667471617ae2f0835fab707baa54b731f991507ebbb55ea85adb12a\n",
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForCausalLM: ['vocab_projector.bias', 'vocab_layer_norm.bias', 'vocab_transform.weight', 'vocab_projector.weight', 'vocab_layer_norm.weight', 'vocab_transform.bias']\n",
      "- This IS expected if you are initializing DistilBertForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForCausalLM were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['distilbert.transformer.layer.1.crossattention.out_lin.weight', 'distilbert.transformer.layer.4.crossattention.q_lin.bias', 'distilbert.transformer.layer.3.crossattention.out_lin.bias', 'distilbert.transformer.layer.2.crossattention.q_lin.weight', 'distilbert.transformer.layer.2.crossattention.v_lin.weight', 'distilbert.transformer.layer.0.crossattention.v_lin.bias', 'distilbert.transformer.layer.4.crossattention.k_lin.weight', 'distilbert.transformer.layer.3.crossattention.k_lin.weight', 'distilbert.transformer.layer.5.crossattention.k_lin.bias', 'distilbert.transformer.layer.3.crossattention.out_lin.weight', 'lm_head.predictions.decoder.weight', 'distilbert.transformer.layer.3.crossattention.k_lin.bias', 'distilbert.transformer.layer.2.crossattention.out_lin.bias', 'distilbert.transformer.layer.4.crossattention.k_lin.bias', 'distilbert.transformer.layer.5.crossattention.q_lin.bias', 'distilbert.transformer.layer.2.crossattention.out_lin.weight', 'distilbert.transformer.layer.1.crossattention.q_lin.bias', 'distilbert.transformer.layer.0.crossattention.v_lin.weight', 'distilbert.transformer.layer.1.crossattention.q_lin.weight', 'distilbert.transformer.layer.0.crossattention.q_lin.bias', 'distilbert.transformer.layer.5.crossattention.q_lin.weight', 'distilbert.transformer.layer.4.crossattention.q_lin.weight', 'distilbert.transformer.layer.0.crossattention.out_lin.weight', 'distilbert.transformer.layer.5.crossattention.k_lin.weight', 'distilbert.transformer.layer.1.crossattention.v_lin.bias', 'distilbert.transformer.layer.3.crossattention.q_lin.bias', 'distilbert.transformer.layer.3.crossattention.q_lin.weight', 'lm_head.predictions.transform.LayerNorm.weight', 'distilbert.transformer.layer.1.crossattention.k_lin.bias', 'distilbert.transformer.layer.4.crossattention.v_lin.bias', 'distilbert.transformer.layer.1.crossattention.out_lin.bias', 'lm_head.decoder.weight', 'distilbert.transformer.layer.3.crossattention.v_lin.weight', 'distilbert.transformer.layer.2.crossattention.v_lin.bias', 'lm_head.predictions.bias', 'distilbert.transformer.layer.2.crossattention.k_lin.weight', 'distilbert.transformer.layer.4.crossattention.out_lin.bias', 'lm_head.predictions.transform.dense.bias', 'distilbert.transformer.layer.1.crossattention.v_lin.weight', 'lm_head.predictions.decoder.bias', 'distilbert.transformer.layer.2.crossattention.k_lin.bias', 'lm_head.predictions.transform.LayerNorm.bias', 'distilbert.transformer.layer.3.crossattention.v_lin.bias', 'distilbert.transformer.layer.5.crossattention.v_lin.bias', 'distilbert.transformer.layer.5.crossattention.v_lin.weight', 'distilbert.transformer.layer.1.crossattention.k_lin.weight', 'distilbert.transformer.layer.0.crossattention.q_lin.weight', 'distilbert.transformer.layer.2.crossattention.q_lin.bias', 'distilbert.transformer.layer.0.crossattention.k_lin.bias', 'distilbert.transformer.layer.5.crossattention.out_lin.bias', 'distilbert.transformer.layer.4.crossattention.out_lin.weight', 'distilbert.transformer.layer.0.crossattention.k_lin.weight', 'lm_head.predictions.transform.dense.weight', 'distilbert.transformer.layer.5.crossattention.out_lin.weight', 'distilbert.transformer.layer.4.crossattention.v_lin.weight', 'distilbert.transformer.layer.0.crossattention.out_lin.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Set `config.is_decoder=True` and `config.add_cross_attention=True` for decoder_config\n"
     ]
    }
   ],
   "source": [
    "from transformers import DistilBertTokenizer, DistilBertModel, DistilBertConfig, EncoderDecoderModel, EncoderDecoderConfig\n",
    "\n",
    "# bert2bert = transformers.AutoModelForSeq2SeqLM.from_pretrained(\"bert-base-uncased\", output_loading_info=True)\n",
    "\n",
    "\n",
    "\n",
    "# config_encoder = DistilBertConfig.from_pretrained('distilbert-base-uncased')\n",
    "# config_decoder = DistilBertConfig.from_pretrained('distilbert-base-uncased')\n",
    "\n",
    "# config = EncoderDecoderConfig.from_encoder_decoder_configs(config_encoder, config_decoder)\n",
    "\n",
    "# distilbert2distilbert = EncoderDecoderModel(config=config)\n",
    "\n",
    "bert2bert = transformers.EncoderDecoderModel.from_encoder_decoder_pretrained(\"distilbert-base-uncased\", \"distilbert-base-uncased\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bcb46f32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set special tokens\n",
    "bert2bert.config.decoder_start_token_id = tokenizer.bos_token_id\n",
    "bert2bert.config.eos_token_id = tokenizer.eos_token_id\n",
    "bert2bert.config.pad_token_id = tokenizer.pad_token_id\n",
    "\n",
    "# sensible parameters for beam search\n",
    "bert2bert.config.vocab_size = bert2bert.config.decoder.vocab_size\n",
    "bert2bert.config.max_length = 142\n",
    "bert2bert.config.min_length = 56\n",
    "bert2bert.config.no_repeat_ngram_size = 3\n",
    "bert2bert.config.early_stopping = True\n",
    "bert2bert.config.length_penalty = 2.0\n",
    "bert2bert.config.num_beams = 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "968655ea",
   "metadata": {},
   "source": [
    "### Evaluation Metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b1bbe583",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rouge import Rouge \n",
    "\n",
    "rouge_scorer = Rouge()\n",
    "\n",
    "def compute_evaluation_metric(pred):\n",
    "    labels_ids = pred.label_ids\n",
    "    pred_ids = pred.predictions\n",
    "\n",
    "    # all unnecessary tokens are removed\n",
    "    pred_str = tokenizer.batch_decode(pred_ids, skip_special_tokens=True)\n",
    "    labels_ids[labels_ids == -100] = tokenizer.pad_token_id\n",
    "    label_str = tokenizer.batch_decode(labels_ids, skip_special_tokens=True)\n",
    "    \n",
    "    score = rouge_scorer.get_scores(label_str, pred_str)\n",
    "    f = score[0][\"rouge-2\"][\"f\"]\n",
    "     \n",
    "    return {\n",
    "        \"rouge2_fmeasure\": f\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1b6ae80",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9a88f91a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "***** Running training *****\n",
      "  Num examples = 144\n",
      "  Num Epochs = 2\n",
      "  Instantaneous batch size per device = 2\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 2\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 144\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='144' max='144' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [144/144 05:48, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Rouge2 Fmeasure</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>nan</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 7\n",
      "  Batch size = 2\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 7\n",
      "  Batch size = 2\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=144, training_loss=0.0, metrics={'train_runtime': 349.5842, 'train_samples_per_second': 0.824, 'train_steps_per_second': 0.412, 'total_flos': 109083781693440.0, 'train_loss': 0.0, 'epoch': 2.0})"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from typing_extensions import Protocol, runtime_checkable\n",
    "from transformers import Seq2SeqTrainingArguments, Seq2SeqTrainer\n",
    "\n",
    "batch_size = 2\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"./\",\n",
    "    evaluation_strategy = \"epoch\",\n",
    "    learning_rate=2e-4,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    weight_decay=0.01,\n",
    "    save_total_limit=3,\n",
    "    num_train_epochs=2,\n",
    "    predict_with_generate=True,\n",
    ")\n",
    "\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=bert2bert,\n",
    "    tokenizer=tokenizer,\n",
    "    args=training_args,\n",
    "    compute_metrics=compute_evaluation_metric,\n",
    "    train_dataset=train_data,\n",
    "    eval_dataset=validation_data,\n",
    ")\n",
    "\n",
    "\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4c13bc09",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to distilbert-v1\n",
      "Configuration saved in distilbert-v1/config.json\n",
      "Model weights saved in distilbert-v1/pytorch_model.bin\n",
      "tokenizer config file saved in distilbert-v1/tokenizer_config.json\n",
      "Special tokens file saved in distilbert-v1/special_tokens_map.json\n"
     ]
    }
   ],
   "source": [
    "dir_name = 'distilbert-v1'\n",
    "trainer.save_model(dir_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03787afa",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d59bf812",
   "metadata": {},
   "outputs": [],
   "source": [
    "# map data correctly\n",
    "def generate_summary(batch):\n",
    "    # Tokenizer will automatically set [BOS] <text> [EOS]\n",
    "    # cut off at BERT max length 512\n",
    "    inputs = tokenizer(batch[\"article\"], padding=\"max_length\", truncation=True, max_length=512, return_tensors=\"pt\")\n",
    "    input_ids = inputs.input_ids\n",
    "    attention_mask = inputs.attention_mask\n",
    "\n",
    "    outputs = model.generate(input_ids, attention_mask=attention_mask)\n",
    "\n",
    "    # all special tokens including will be removed\n",
    "    output_str = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "\n",
    "    batch[\"pred\"] = output_str\n",
    "\n",
    "    return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ff3f9e38",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset cnn_dailymail (/Users/jeroen/.cache/huggingface/datasets/cnn_dailymail/3.0.0/3.0.0/3cb851bf7cf5826e45d49db2863f627cba583cbc32342df7349dfe6c38060234)\n",
      "loading configuration file distilbert-v1/config.json\n",
      "Model config EncoderDecoderConfig {\n",
      "  \"architectures\": [\n",
      "    \"EncoderDecoderModel\"\n",
      "  ],\n",
      "  \"decoder\": {\n",
      "    \"_name_or_path\": \"distilbert-base-uncased\",\n",
      "    \"activation\": \"gelu\",\n",
      "    \"add_cross_attention\": true,\n",
      "    \"architectures\": [\n",
      "      \"DistilBertForMaskedLM\"\n",
      "    ],\n",
      "    \"attention_dropout\": 0.1,\n",
      "    \"bad_words_ids\": null,\n",
      "    \"bos_token_id\": null,\n",
      "    \"chunk_size_feed_forward\": 0,\n",
      "    \"cross_attention_hidden_size\": null,\n",
      "    \"decoder_start_token_id\": null,\n",
      "    \"dim\": 768,\n",
      "    \"diversity_penalty\": 0.0,\n",
      "    \"do_sample\": false,\n",
      "    \"dropout\": 0.1,\n",
      "    \"early_stopping\": false,\n",
      "    \"encoder_no_repeat_ngram_size\": 0,\n",
      "    \"eos_token_id\": null,\n",
      "    \"finetuning_task\": null,\n",
      "    \"forced_bos_token_id\": null,\n",
      "    \"forced_eos_token_id\": null,\n",
      "    \"hidden_dim\": 3072,\n",
      "    \"id2label\": {\n",
      "      \"0\": \"LABEL_0\",\n",
      "      \"1\": \"LABEL_1\"\n",
      "    },\n",
      "    \"initializer_range\": 0.02,\n",
      "    \"is_decoder\": true,\n",
      "    \"is_encoder_decoder\": false,\n",
      "    \"label2id\": {\n",
      "      \"LABEL_0\": 0,\n",
      "      \"LABEL_1\": 1\n",
      "    },\n",
      "    \"layer_norm_eps\": 1e-12,\n",
      "    \"length_penalty\": 1.0,\n",
      "    \"max_length\": 20,\n",
      "    \"max_position_embeddings\": 512,\n",
      "    \"min_length\": 0,\n",
      "    \"model_type\": \"distilbert\",\n",
      "    \"n_heads\": 12,\n",
      "    \"n_layers\": 6,\n",
      "    \"no_repeat_ngram_size\": 0,\n",
      "    \"num_beam_groups\": 1,\n",
      "    \"num_beams\": 1,\n",
      "    \"num_return_sequences\": 1,\n",
      "    \"output_attentions\": false,\n",
      "    \"output_hidden_states\": false,\n",
      "    \"output_scores\": false,\n",
      "    \"pad_token_id\": 0,\n",
      "    \"prefix\": null,\n",
      "    \"problem_type\": null,\n",
      "    \"pruned_heads\": {},\n",
      "    \"qa_dropout\": 0.1,\n",
      "    \"remove_invalid_values\": false,\n",
      "    \"repetition_penalty\": 1.0,\n",
      "    \"return_dict\": true,\n",
      "    \"return_dict_in_generate\": false,\n",
      "    \"sep_token_id\": null,\n",
      "    \"seq_classif_dropout\": 0.2,\n",
      "    \"sinusoidal_pos_embds\": false,\n",
      "    \"task_specific_params\": null,\n",
      "    \"temperature\": 1.0,\n",
      "    \"tie_encoder_decoder\": false,\n",
      "    \"tie_weights_\": true,\n",
      "    \"tie_word_embeddings\": true,\n",
      "    \"tokenizer_class\": null,\n",
      "    \"top_k\": 50,\n",
      "    \"top_p\": 1.0,\n",
      "    \"torch_dtype\": null,\n",
      "    \"torchscript\": false,\n",
      "    \"transformers_version\": \"4.17.0.dev0\",\n",
      "    \"typical_p\": 1.0,\n",
      "    \"use_bfloat16\": false,\n",
      "    \"vocab_size\": 30522\n",
      "  },\n",
      "  \"decoder_start_token_id\": 101,\n",
      "  \"early_stopping\": true,\n",
      "  \"encoder\": {\n",
      "    \"_name_or_path\": \"distilbert-base-uncased\",\n",
      "    \"activation\": \"gelu\",\n",
      "    \"add_cross_attention\": false,\n",
      "    \"architectures\": [\n",
      "      \"DistilBertForMaskedLM\"\n",
      "    ],\n",
      "    \"attention_dropout\": 0.1,\n",
      "    \"bad_words_ids\": null,\n",
      "    \"bos_token_id\": null,\n",
      "    \"chunk_size_feed_forward\": 0,\n",
      "    \"cross_attention_hidden_size\": null,\n",
      "    \"decoder_start_token_id\": null,\n",
      "    \"dim\": 768,\n",
      "    \"diversity_penalty\": 0.0,\n",
      "    \"do_sample\": false,\n",
      "    \"dropout\": 0.1,\n",
      "    \"early_stopping\": false,\n",
      "    \"encoder_no_repeat_ngram_size\": 0,\n",
      "    \"eos_token_id\": null,\n",
      "    \"finetuning_task\": null,\n",
      "    \"forced_bos_token_id\": null,\n",
      "    \"forced_eos_token_id\": null,\n",
      "    \"hidden_dim\": 3072,\n",
      "    \"id2label\": {\n",
      "      \"0\": \"LABEL_0\",\n",
      "      \"1\": \"LABEL_1\"\n",
      "    },\n",
      "    \"initializer_range\": 0.02,\n",
      "    \"is_decoder\": false,\n",
      "    \"is_encoder_decoder\": false,\n",
      "    \"label2id\": {\n",
      "      \"LABEL_0\": 0,\n",
      "      \"LABEL_1\": 1\n",
      "    },\n",
      "    \"layer_norm_eps\": 1e-12,\n",
      "    \"length_penalty\": 1.0,\n",
      "    \"max_length\": 20,\n",
      "    \"max_position_embeddings\": 512,\n",
      "    \"min_length\": 0,\n",
      "    \"model_type\": \"distilbert\",\n",
      "    \"n_heads\": 12,\n",
      "    \"n_layers\": 6,\n",
      "    \"no_repeat_ngram_size\": 0,\n",
      "    \"num_beam_groups\": 1,\n",
      "    \"num_beams\": 1,\n",
      "    \"num_return_sequences\": 1,\n",
      "    \"output_attentions\": false,\n",
      "    \"output_hidden_states\": false,\n",
      "    \"output_scores\": false,\n",
      "    \"pad_token_id\": 0,\n",
      "    \"prefix\": null,\n",
      "    \"problem_type\": null,\n",
      "    \"pruned_heads\": {},\n",
      "    \"qa_dropout\": 0.1,\n",
      "    \"remove_invalid_values\": false,\n",
      "    \"repetition_penalty\": 1.0,\n",
      "    \"return_dict\": true,\n",
      "    \"return_dict_in_generate\": false,\n",
      "    \"sep_token_id\": null,\n",
      "    \"seq_classif_dropout\": 0.2,\n",
      "    \"sinusoidal_pos_embds\": false,\n",
      "    \"task_specific_params\": null,\n",
      "    \"temperature\": 1.0,\n",
      "    \"tie_encoder_decoder\": false,\n",
      "    \"tie_weights_\": true,\n",
      "    \"tie_word_embeddings\": true,\n",
      "    \"tokenizer_class\": null,\n",
      "    \"top_k\": 50,\n",
      "    \"top_p\": 1.0,\n",
      "    \"torch_dtype\": null,\n",
      "    \"torchscript\": false,\n",
      "    \"transformers_version\": \"4.17.0.dev0\",\n",
      "    \"typical_p\": 1.0,\n",
      "    \"use_bfloat16\": false,\n",
      "    \"vocab_size\": 30522\n",
      "  },\n",
      "  \"eos_token_id\": 102,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"length_penalty\": 2.0,\n",
      "  \"max_length\": 142,\n",
      "  \"min_length\": 56,\n",
      "  \"model_type\": \"encoder-decoder\",\n",
      "  \"no_repeat_ngram_size\": 3,\n",
      "  \"num_beams\": 4,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": null,\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading weights file distilbert-v1/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing EncoderDecoderModel.\n",
      "\n",
      "All the weights of EncoderDecoderModel were initialized from the model checkpoint at distilbert-v1.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use EncoderDecoderModel for predictions without further training.\n",
      "100%|███████████████████████████████████████████| 16/16 [06:24<00:00, 24.05s/ba]\n"
     ]
    }
   ],
   "source": [
    "test_data = datasets.load_dataset(\"cnn_dailymail\", \"3.0.0\", split=\"test\")\n",
    "test_data = test_data.select(range(32))\n",
    "model = transformers.EncoderDecoderModel.from_pretrained(dir_name)\n",
    "\n",
    "results = test_data.map(generate_summary, batched=True, batch_size=batch_size)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fcf74e7",
   "metadata": {},
   "source": [
    "## Displaying Results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6eb88dbd",
   "metadata": {},
   "source": [
    "### Util function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc5b9178",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Credits tot Mauro Di Pietro\n",
    "# https://towardsdatascience.com/text-summarization-with-nlp-textrank-vs-seq2seq-vs-bart-474943efeb09\n",
    "\n",
    "import re\n",
    "import difflib\n",
    "import nltk\n",
    "# nltk.download()\n",
    "\n",
    "'''\n",
    "Find the matching substrings in 2 strings.\n",
    ":parameter\n",
    "    :param a: string - raw text\n",
    "    :param b: string - raw text\n",
    ":return\n",
    "    2 lists used in to display matches\n",
    "'''\n",
    "def utils_split_sentences(a, b):\n",
    "    ## find clean matches\n",
    "    match = difflib.SequenceMatcher(isjunk=None, a=a, b=b, autojunk=True)\n",
    "    lst_match = [block for block in match.get_matching_blocks() if block.size > 20]\n",
    "    \n",
    "    ## difflib didn't find any match\n",
    "    if len(lst_match) == 0:\n",
    "        lst_a, lst_b = nltk.sent_tokenize(a), nltk.sent_tokenize(b)\n",
    "    \n",
    "    ## work with matches\n",
    "    else:\n",
    "        first_m, last_m = lst_match[0], lst_match[-1]\n",
    "\n",
    "        ### a\n",
    "        string = a[0 : first_m.a]\n",
    "        lst_a = [t for t in nltk.sent_tokenize(string)]\n",
    "        for n in range(len(lst_match)):\n",
    "            m = lst_match[n]\n",
    "            string = a[m.a : m.a+m.size]\n",
    "            lst_a.append(string)\n",
    "            if n+1 < len(lst_match):\n",
    "                next_m = lst_match[n+1]\n",
    "                string = a[m.a+m.size : next_m.a]\n",
    "                lst_a = lst_a + [t for t in nltk.sent_tokenize(string)]\n",
    "            else:\n",
    "                break\n",
    "        string = a[last_m.a+last_m.size :]\n",
    "        lst_a = lst_a + [t for t in nltk.sent_tokenize(string)]\n",
    "\n",
    "        ### b\n",
    "        string = b[0 : first_m.b]\n",
    "        lst_b = [t for t in nltk.sent_tokenize(string)]\n",
    "        for n in range(len(lst_match)):\n",
    "            m = lst_match[n]\n",
    "            string = b[m.b : m.b+m.size]\n",
    "            lst_b.append(string)\n",
    "            if n+1 < len(lst_match):\n",
    "                next_m = lst_match[n+1]\n",
    "                string = b[m.b+m.size : next_m.b]\n",
    "                lst_b = lst_b + [t for t in nltk.sent_tokenize(string)]\n",
    "            else:\n",
    "                break\n",
    "        string = b[last_m.b+last_m.size :]\n",
    "        lst_b = lst_b + [t for t in nltk.sent_tokenize(string)]\n",
    "    \n",
    "    return lst_a, lst_b\n",
    "\n",
    "\n",
    "'''\n",
    "Highlights the matched strings in text.\n",
    ":parameter\n",
    "    :param a: string - raw text\n",
    "    :param b: string - raw text\n",
    "    :param both: bool - search a in b and, if True, viceversa\n",
    "    :param sentences: bool - if False matches single words\n",
    ":return\n",
    "    text html, it can be visualized on notebook with display(HTML(text))\n",
    "'''\n",
    "def display_string_matching(a, b, both=True, sentences=True, titles=[]):\n",
    "    if sentences is True:\n",
    "        lst_a, lst_b = utils_split_sentences(a, b)\n",
    "    else:\n",
    "        lst_a, lst_b = a.split(), b.split()       \n",
    "    \n",
    "    ## highlight a\n",
    "    first_text = []\n",
    "    for i in lst_a:\n",
    "        if re.sub(r'[^\\w\\s]', '', i.lower()) in [re.sub(r'[^\\w\\s]', '', z.lower()) for z in lst_b]:\n",
    "            first_text.append('<span style=\"background-color:rgba(255,215,0,0.3);\">' + i + '</span>')\n",
    "        else:\n",
    "            first_text.append(i)\n",
    "    first_text = ' '.join(first_text)\n",
    "    \n",
    "    ## highlight b\n",
    "    second_text = []\n",
    "    if both is True:\n",
    "        for i in lst_b:\n",
    "            if re.sub(r'[^\\w\\s]', '', i.lower()) in [re.sub(r'[^\\w\\s]', '', z.lower()) for z in lst_a]:\n",
    "                second_text.append('<span style=\"background-color:rgba(255,215,0,0.3);\">' + i + '</span>')\n",
    "            else:\n",
    "                second_text.append(i)\n",
    "    else:\n",
    "        second_text.append(b) \n",
    "    second_text = ' '.join(second_text)\n",
    "    \n",
    "    ## concatenate\n",
    "    if len(titles) > 0:\n",
    "        first_text = \"<strong>\"+titles[0]+\"</strong><br>\"+first_text\n",
    "    if len(titles) > 1:\n",
    "        second_text = \"<strong>\"+titles[1]+\"</strong><br>\"+second_text\n",
    "    else:\n",
    "        second_text = \"---\"*65+\"<br><br>\"+second_text\n",
    "    final_text = first_text +'<br><br>'+ second_text\n",
    "    return final_text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40d45a7f",
   "metadata": {},
   "source": [
    "## Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3a213cb",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "from rouge import Rouge \n",
    "\n",
    "rouge_new = Rouge()\n",
    "\n",
    "for i in range(10):\n",
    "    article = results[\"article\"][i]\n",
    "    highlight = results[\"highlights\"][i]\n",
    "    prediction = results[\"pred\"][i]\n",
    "    score = rouge_new.get_scores(highlight, prediction)\n",
    "    \n",
    "    rouge_1_f = score[0][\"rouge-1\"][\"f\"] * 100\n",
    "    rouge_2_f = score[0][\"rouge-2\"][\"f\"] * 100\n",
    "    s = f\"rouge-1: {rouge_1_f}, rouge-2:  {rouge_2_f}\" \n",
    "    \n",
    "#     match_article_prediction = display_string_matching(article, prediction, \n",
    "#                                                        both=True, sentences=False,\n",
    "#                                                       titles=[\"Article\", \"Predicted Summary\"])\n",
    "    \n",
    "    match_summary = display_string_matching(highlight, prediction, both=True, \n",
    "                                    sentences=False, \n",
    "                                    titles=[\"Real Summary\", f\"Predicted Summary ({s})\"])\n",
    "    \n",
    "    display(HTML(match_summary))\n",
    "    print(\"---\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
